<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Can DeepSeek-R1 improve the performance of one of the worlds oldest codebases?</title>

    <script>
    (function() {
        // Set initial theme as early as possible to avoid flash. Use stored preference if present,
        // otherwise default to the user's system preference (prefers-color-scheme).
        var stored = null;
        try { stored = localStorage.getItem('theme'); } catch (e) { stored = null; }
        var theme;
        if (stored === 'light' || stored === 'dark') {
            theme = stored;
        } else if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
            theme = 'dark';
        } else {
            theme = 'light';
        }
        document.documentElement.setAttribute('data-theme', theme);
    })();
    </script>

    <!-- Facebook Meta tags -->
    <meta property="og:title" content="Can DeepSeek-R1 improve the performance of one of the worlds oldest codebases?">
    <meta property="og:description" content="Exploring whether reasoning models can perform complex optimizations of old code">
    <meta property="og:image" content="https://tonybaloney.github.io/img/posts/snail.jpg">
    <meta property="og:url" content="https://tonybaloney.github.io/posts/can-reasoning-models-optimize-20-year-old-code.html">
    <!-- Twitter Meta Tags -->
    <meta name="twitter:title" content="Can DeepSeek-R1 improve the performance of one of the worlds oldest codebases?">
    <meta name="twitter:description" content="Exploring whether reasoning models can perform complex optimizations of old code">
    <meta name="twitter:image" content="https://tonybaloney.github.io/img/posts/snail.jpg">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@anthonypjshaw">

    <!-- Bootstrap Core CSS -->
    <link href="/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Theme CSS -->
    <link href="/css/clean-blog.min.css" rel="stylesheet">

    <style>
    /* Simple CSS variable-based theme with a dark variant. Adjusts background, text, nav, footer and code blocks. */
    :root {
        --bg: #ffffff;
        --text: #222222;
        --muted: #777777;
        --link: #337ab7;
        --navbar-bg: rgba(255,255,255,0.95);
        --navbar-text: #222222;
        --code-bg: #f6f8fa;
        --pre-color: #2e2e2e;
    }
    [data-theme='dark'] {
        --bg: #0b0f17;
        --text: #e6edf3;
        --muted: #9aa0a6;
        --link: #8fc7ff;
        --navbar-bg: rgba(10,10,10,0.95);
        --navbar-text: #e6edf3;
        --code-bg: #282c34;
        --pre-color: #abb2bf;
    }

    html, body {
        background-color: var(--bg) !important;
        color: var(--text) !important;
    }

    a, a:link, a:visited { color: var(--link) !important; }

    /* Navbar */
    .navbar-default { background-color: var(--navbar-bg) !important; border-color: transparent !important; }
    .navbar-default .navbar-nav>li>a { color: var(--navbar-text) !important; }

    /* Header */
    .intro-header .site-heading h1, .intro-header .site-heading .subheading { color: var(--text) !important; text-shadow: none !important; }

    /* Footer */
    footer { color: var(--muted) !important; }

    /* Code / highlight.js */
    pre, code, .hljs { background: var(--code-bg) !important; color: var(--pre-color) !important; }
    .hljs { padding: 1em; display:block; overflow-x:auto; }

    /* Toggle button sizing */
    #theme-toggle { display:inline-flex; align-items:center; justify-content:center; padding:6px; }
    #theme-toggle i { font-size:1.6em; }

    /* A few token overrides for dark mode highlighting (when highlight.js default theme is used) */
    [data-theme='dark'] .hljs { background: #282c34 !important; color: #abb2bf !important; }
    [data-theme='dark'] .hljs-keyword, [data-theme='dark'] .hljs-selector-tag, [data-theme='dark'] .hljs-literal { color: #c678dd; }
    [data-theme='dark'] .hljs-string, [data-theme='dark'] .hljs-title, [data-theme='dark'] .hljs-name { color: #98c379; }
    [data-theme='dark'] .hljs-comment, [data-theme='dark'] .hljs-quote { color: #5c6370; font-style: italic; }

    /* Footer icons: make sure stacked icons render white on dark theme */
    [data-theme='dark'] footer .fa-stack .fa-inverse { color: #ffffff !important; }
    [data-theme='dark'] footer .fa-stack .fa-circle { color: rgba(255,255,255,0.08) !important; }

    /* Related posts card styling */
    .related-posts { margin-top: 2rem; }
    .related-posts .related-box { border: 1px solid rgba(0,0,0,0.06); padding: 12px; border-radius: 6px; background: var(--bg); color: var(--text); }
    [data-theme='dark'] .related-posts .related-box { border-color: rgba(255,255,255,0.06); background: rgba(255,255,255,0.02); }
    .related-posts .related-box img { width:100%; height:200px; border-radius:4px;  }
    .related-posts .related-box .title { font-weight:700; margin-top:8px; }
    .related-posts .related-box .subheading { color: var(--muted); font-size:0.95em; }
    .related-posts .related-box .post-meta { font-size:0.9em; margin-top:6px; }
    </style>

    <!-- Custom Fonts -->
    <link href="/vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/styles/default.min.css">
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/highlight.min.js"></script>

    <script charset="UTF-8" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/languages/x86asm.min.js"></script>
    <script charset="UTF-8" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/languages/python.min.js"></script>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
</head>

<body>

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-custom navbar-fixed-top">
        <div class="container-fluid">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    Menu <i class="fa fa-bars"></i>
                </button>

            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/#">Home</a>
                    </li>
                    <li>
                        <a href="/#projects">Projects</a>
                    </li>
                    <li>
                        <a href="/#contributions">Contributions</a>
                    </li>
                    <li>
                        <a href="/#courses">Courses</a>
                    </li>
                    <li>
                        <a href="/#podcasts">Podcasts</a>
                    </li>
                    <li>
                        <a href="/#talks">Talks</a>
                    </li>
                    <li>
                        <a href="/#blog">Blog</a>
                    </li>
                    <li>
                        <a href="https://twitter.com/anthonypjshaw"><i class='fa fa-2x fa-twitter'></i></a>
                    </li>
                    <li>
                        <a href="https://github.com/tonybaloney"><i class='fa fa-2x fa-github'></i></a>
                    </li>
                    <li>
                        <a href="https://www.youtube.com/c/AnthonyShaw"><i class='fa fa-2x fa-youtube-play'></i></a>
                    </li>
                    <li>
                        <a href="/rss.xml"><i class='fa fa-2x fa-rss-square'></i></a>
                    </li>
                    <li>
                        <a href="#" id="theme-toggle" aria-label="Toggle dark mode" title="Toggle dark mode"><i class="fa fa-2x fa-moon-o" aria-hidden="true"></i></a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Page Header -->
    <!-- Set your background image for this header on the line below. -->
    <header class="intro-header" style="background-image: url('/img/posts/snail.jpg')">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <div class="site-heading">
                        <h1>Can DeepSeek-R1 improve the performance of one of the worlds oldest codebases?</h1>
                        <hr class="small">
                        <span class="subheading">by Anthony Shaw, January 30, 2025</span>
                    </div>
                </div>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <div class="container">
        <div class="row">
            <div class="col-md-12">
                <p>For a while now I&rsquo;ve been trying to use LLMs to optimize code. I&rsquo;ve given many talks on performance in dynamic languages and published a thesis on this last year. Chasing code performance comes down to some key steps:</p>
<ol>
<li>Understanding how the code runs</li>
<li>Understanding how the system runs it (memory, disk, CPU caching)</li>
<li>Understanding how to profile the code and understand where it is slow</li>
<li>Understanding tricks and patterns to improve it</li>
</ol>
<p>LLMs are great at the fourth step. They have seen more code than 1000 programmers will ever see in a lifetime and they can recognize patterns in all commonly used languages. They also know the theory of how code runs. They&rsquo;ve &ldquo;read&rdquo; all the Computer Science textbooks and know all the algorithms.</p>
<p>The secret sauce is knowing what to optimize and being able to apply experience and knowledge to experimenting.</p>
<p>Take these two recent examples that I think an LLM would never have come up with if you gave it the code and said &ldquo;make this faster&rdquo;:</p>
<ol>
<li>OpenAI&rsquo;s Python SDK has a performance option when using the embeddings API to use the NumPy package to decode the floating point values. <a href="https://github.com/openai/openai-python/pull/2060">I submitted an alternative approach</a> built into Python which is 20% faster, a handful of lines of code and for the average user is 420% faster. </li>
<li>Ken Jin submitted a proposal to make CPython 10% faster by <a href="https://github.com/python/cpython/issues/128563">switching the interpreter to tail-calling</a></li>
</ol>
<p>When I ask LLMs to optimize code they tend-to give some generic text-book guidance on optimization. I just tried GPT-4o with the instruction &ldquo;make this code faster&rdquo; for a Python function, it said:</p>
<pre><code>To make this code faster, we can consider the following optimizations:

Avoid repeated attribute lookups: Store frequently accessed attributes in local variables.
Optimize the loop: If possible, optimize the loop to reduce overhead.
</code></pre>
<p><code>If possible, optimize the loop to reduce overhead.</code> - well thank you for that insight.</p>
<p>The key to being successful with LLMs is to provide <strong>context</strong> and <strong>clear instructions</strong>. But, there is so much implicit knowledge in how to make a function or block of code faster. This information either doesn&rsquo;t fit in the context window of ~4000 tokens, or the instructions are so complicated the LLM can&rsquo;t handle them all and skips over important ones to produce something slower or incorrect.</p>
<p>This is where reasoning models come in.</p>
<p>As an experiment I wanted to see how the DeepSeek-R1 reasoning model could handle a complex optimization problem.</p>
<p>To make the experiment even more interesting, I picked a block of code deep inside Python itself that handles the arithmetic of large multiplications and large exponent calculations (e.g. 2**34443).</p>
<p>If the AI makes it faster and doesn&rsquo;t break Python, it would <strong>make math faster in Python.</strong> A tempting prize.</p>
<p>I saw a similar post from the Ollama.cpp project where DeepSeek-R1 was used to apply optimizations to a codebase. The developer shared their prompt and it gave a clear example of the output. Their prompt was different to what I&rsquo;m trying here because it gave a very clear templated example.</p>
<h2 id="how-is-a-reasoning-model-different-to-other-llms">How is a reasoning model different to other LLMs?<a class="headerlink" href="#how-is-a-reasoning-model-different-to-other-llms" title="Permanent link">&para;</a></h2>
<p>Reasoning models are types of LLMs specialized for complex tasks. Reasoning models typically perform significantly better on Math tests or with brain teasers than traditional LLMs. Reasoning models can be given long lists of constraints and will explore different paths recursively to solve a problem.</p>
<p>DeepSeek-R1 is not the only reasoning model. OpenAI&rsquo;s o1, o1-mini, and o3 models are reasoning models. Qwen <a href="https://qwenlm.github.io/blog/qvq-72b-preview/">QVQ</a> is a reasoning model from Alibaba that I was playing with in <a href="https://tonybaloney.github.io/posts/the-big-fib-can-you-prompt-llms-to-tell-the-truth.html">December to try and solve a game of Set</a>.</p>
<h2 id="the-problem-statement">The problem statement<a class="headerlink" href="#the-problem-statement" title="Permanent link">&para;</a></h2>
<p>For DeepSeek-R1 I wanted it to optimize some C code inside Python itself. This code is old and written by some very smart people, many of whom have created the knowledge the LLMs are trying to imitate.</p>
<p>I wanted the LLM to reason why the following code wasn&rsquo;t optimized by the compiler and whether the code itself could be changed, <strong>without</strong> changing. The new code must be equivalent, and it must meet a long list of constraints.</p>
<p>This is a hard problem that goes well beyond <em>&ldquo;optimize the loop to reduce overhead.&rdquo;</em></p>
<p>Here is the code:</p>
<pre><code class="c">static digit
v_iadd(digit *x, Py_ssize_t m, digit *y, Py_ssize_t n)
{
    Py_ssize_t i;
    digit carry = 0;

    assert(m &gt;= n);
    for (i = 0; i &lt; n; ++i) {
        carry += x[i] + y[i];
        x[i] = carry &amp; PyLong_MASK;
        carry &gt;&gt;= PyLong_SHIFT;
        assert((carry &amp; 1) == carry);
    }
    for (; carry &amp;&amp; i &lt; m; ++i) {
        carry += x[i];
        x[i] = carry &amp; PyLong_MASK;
        carry &gt;&gt;= PyLong_SHIFT;
        assert((carry &amp; 1) == carry);
    }
    return carry;
}
</code></pre>

<p>This function adds two vectors of <code>digit</code> where the length of vector <code>x</code> is <code>m</code> and the length of vector <code>y</code> is <code>n</code>. This function does inplace addition (so the result is written to the vector <code>x</code>) and it returns any carried digit if it exceeds the existing length of <code>x</code>. To make things more complicated, Python doesn&rsquo;t use base-10 to store numbers, so <code>digit</code> is not a value <code>0-9</code> as you might expect. This creates a harder problem for LLMs who may have seen hundreds of similar bits of inplace vector add functions with base-10.</p>
<p>The optimization path I&rsquo;m seeking is for the LLVM Clang compiler to vectorize the instructions inside the loop to more efficient vector arithmetic. The problem with vectorization optimizations is that they require a lot of conditions to be met before they can work effectively. </p>
<p>I wrote up all of those conditions into a prompt as the set of instructions for the LLM along with it&rsquo;s task:</p>
<pre><code class="markdown">Your job is to suggest optimizations to C code by finding loops which could be vectorized.

The clang compiler has decided that this loop cannot be vectorized, because it fails to meet one of the following rules. Your job is to work out which of these rules is not met in the code, then suggest an alternative block of code that would meet these requirements. The vectorizer will compile SIMD instructions into the program. This is efficient when the instructions in the code are the same for each iteration and they can be executed in parallel by multiple cores. If the loop contains complex branching logic, then it cannot be vectorized. If you decide that a loop cannot be vectorized without simple changes, explain why. 

Here are the rules and constraints for the LLVM vectorizer algorithm:

# Loop Vectorizer Rules

- The loop trip count (number of cycles) at entry to the loop at runtime.
- The number of loop cycles can not change once entering the loop. e.g. if there is a conditional continue statement, it cannot be vectorized.
- The loop must have a single entry and exit point. e.g. If there are break statements in the code, it has multiple exit points.

- The Loop Vectorizer operates on loops and widens instructions to operate on multiple consecutive iterations.
- The Loop Vectorizer supports loops with unknown trip count at compile time. The trip count does not need to be constant.
- The Loop Vectorizer can vectorize loops with runtime checks of pointers.
- The Loop Vectorizer can vectorize reductions (e.g., sum, product, XOR, AND, OR).
- The Loop Vectorizer can vectorize inductions (e.g., saving the value of the induction variable into an array).
- The Loop Vectorizer can vectorize if-conversions (e.g., flattening IF statements).
- The Loop Vectorizer can vectorize pointer induction variables (e.g., using C++ iterators).
- The Loop Vectorizer can vectorize reverse iterators (e.g., counting backwards).
- The Loop Vectorizer can vectorize scatter/gather operations.
- The Loop Vectorizer can vectorize mixed types (e.g., combining integers and floats).
- The Loop Vectorizer can vectorize global structures alias analysis (e.g., accessing global structures).
- The Loop Vectorizer can vectorize certain function calls (e.g., intrinsic math functions).

# SLP Vectorizer Rules

- The SLP Vectorizer combines similar independent instructions into vector instructions.
- The SLP Vectorizer processes code bottom-up, across basic blocks, in search of scalars to combine.
- The SLP Vectorizer can vectorize memory accesses, arithmetic operations, comparison operations, and PHI-nodes.

# Constraints

- Loops with complicated control flow, unvectorizable types, and unvectorizable calls cannot be vectorized.
- The Loop Vectorizer uses a cost model to decide when it is profitable to unroll loops.
- The Loop Vectorizer may not be able to vectorize math library functions that access external state (e.g., &quot;errno&quot;).
- The Loop Vectorizer may not be able to vectorize loops with undefined behavior.

This is the C code to evaluate:
{{ code }}
</code></pre>

<p>Whatever code it produced, I would copy + paste it into CPython, see if it compiles and then run the CPython test suite to see if it worked.</p>
<h2 id="phase-0-trying-a-70b-reasoning-model">Phase 0 : Trying a 70B reasoning model<a class="headerlink" href="#phase-0-trying-a-70b-reasoning-model" title="Permanent link">&para;</a></h2>
<p>The first model I tried was <a href="">Meta&rsquo;s Llama 3.3 70B</a>. &ldquo;Instruct offers enhanced reasoning, math, and instruction following with performance comparable to Llama 3.1 405B.&rdquo;</p>
<p>You can run Llama 3.3 70B directly in <a href="https://github.com/marketplace/models/azureml-meta/Llama-3-3-70B-Instruct/playground">GitHub Models</a> with a GitHub Account for free (which is what I did): </p>
<p><img alt="Llama 3.3" class="img-responsive center-block" src="/img/posts/llama-github-models.png"></p>
<pre><code>Analysis of the Provided C Code The given C code is a function v_iadd that performs an addition operation on two arrays of digits (x and y) with lengths m and n, respectively. The function returns the carry after the addition.

Vectorization Analysis The code contains two loops:

The first loop iterates over the elements of y and adds corresponding elements from x and y. The carry is updated and propagated to the next iteration. The second loop continues the addition operation if there is a remaining carry after the first loop. It iterates over the remaining elements of x and updates the carry. Vectorization Constraints The Loop Vectorizer Rules and Constraints suggest that the vectorizer can handle loops with unknown trip count, runtime checks, reductions, inductions, and if-conversions. However, the code has some limitations that might prevent vectorization:

The loops have a complex control flow due to the carry propagation and the conditional continuation of the second loop. The loops contain assertions, which might be considered as undefined behavior if the conditions are not met. Non-Vectorizable Rule The rule that is not met in the code is:

The loop must have a single entry and exit point. The second loop has a conditional entry point (for (; carry &amp;&amp; i &lt; m; ++i)), which means that the loop might not be executed if the carry is zero. This conditional entry point creates multiple exit points, making the loop non-vectorizable.

Suggested Optimization To optimize the code for vectorization, we can consider the following alternatives:

Unroll the loops: If the lengths of the arrays are known at compile time, we can unroll the loops to reduce the number of iterations and simplify the control flow. Use SIMD-friendly data structures: If possible, we can use SIMD-friendly data structures, such as arrays of structs or structs of arrays, to improve the memory access patterns and reduce the number of loads and stores. Use intrinsic functions: We can use intrinsic functions, such as those provided by the compiler or libraries like SIMDPP, to perform the addition operation in a vectorized manner. Here's an example of how the code could be modified to use SIMD-friendly data structures and intrinsic functions:
</code></pre>

<p>Llama 3.3 Instruct then produced the following code:</p>
<pre><code class="c">#include &lt;immintrin.h&gt;

static digit
v_iadd(digit *x, Py_ssize_t m, digit *y, Py_ssize_t n)
{
    Py_ssize_t i;
    digit carry = 0;

    assert(m &gt;= n);
    for (i = 0; i &lt; n; i += 4) {
        __m128i x_vec = _mm_loadu_si128((__m128i *)(x + i));
        __m128i y_vec = _mm_loadu_si128((__m128i *)(y + i));
        __m128i sum_vec = _mm_add_epi32(x_vec, y_vec);
        __m128i carry_vec = _mm_srli_epi32(sum_vec, PyLong_SHIFT);
        carry += _mm_extract_epi32(carry_vec, 0);
        _mm_storeu_si128((__m128i *)(x + i), sum_vec);
    }
    for (; carry &amp;&amp; i &lt; m; ++i) {
        carry += x[i];
        x[i] = carry &amp; PyLong_MASK;
        carry &gt;&gt;= PyLong_SHIFT;
    }
    return carry;
}
</code></pre>

<p>If you squint this code looks really smart and efficient. It&rsquo;s replaced first loop with another that works in steps of 4 and uses a 128-bit instruction library. </p>
<p>There are 2 major issues with it&rsquo;s answer. Firstly, it missed the 128-bit equivalent to <code>carry &amp; PyLong_MASK</code> so the answer will almost always be wrong. I told Instruct about the first issue (I&rsquo;ll come to the second in a moment). </p>
<p>Instruct gives me the corrected code and explains it&rsquo;s changes:</p>
<pre><code>"In this corrected code, I've added a line to mask the sum_vec with PyLong_MASK before storing it back into the x array. This ensures that the result of the addition is properly masked to fit within the digit type."
</code></pre>
<p>The corrected code is now numerically correct. But it took a human to intervene and explain it was wrong to get there. The second issue is what whilst it is efficient to do the arithmetic of 32-bit integers in blocks of 4 using specialised 128-bit instructions, <code>n</code> is not nessecarily a factor of 4.</p>
<p>So what happens when it isn&rsquo;t a factor of 4? I said I would just compile it and find out, so:</p>
<p><img alt="Llama Instruct Crash 1" class="img-responsive center-block" src="/img/posts/llama-crash-1.jpg"></p>
<p>The extra numbers from the 128-bit numbers are written into someone else&rsquo;s memory space, causing Python to crash every time it multiplies big numbers together. Or worse, have undefined behaviour.</p>
<p>I prompted it &ldquo;What about overflow errors?&rdquo;, it reasoned (incorrectly) why that might be happening, produced some more code that crashed and then I pushed it further &ldquo;this is still overflowing&rdquo; and it produced even more complicated (and wrong) code.</p>
<p>So the first test wasn&rsquo;t successful, but it could be onto something if it were specialized where <code>n</code> is a multiple of 4.</p>
<h2 id="phase-1-trying-the-little-model-that-could">Phase 1 : Trying the little model that could<a class="headerlink" href="#phase-1-trying-the-little-model-that-could" title="Permanent link">&para;</a></h2>
<p>Next I wanted to try a distilled DeepSeek-R1 model. These are available on Ollama so that you can download them and run them on normal hardware. I have a 4-year old laptop running Windows and an onboard Nvidia GPU. I can run both the 7B and 8B models on this hardware.</p>
<p>To test I used the [AI Toolkit for VS Code] interface to ollama (screenshot.)</p>
<h2 id="phase-2-trying-a-larger-model">Phase 2 : Trying a larger model<a class="headerlink" href="#phase-2-trying-a-larger-model" title="Permanent link">&para;</a></h2>
<ul>
<li>
<p>Explain what happens with the 14B model</p>
</li>
<li>
<p>Show the new code</p>
</li>
<li>Show the benchmark</li>
</ul>
<h2 id="phase-3-running-the-full-371b-parameter-model">Phase 3 : Running the full 371B parameter model<a class="headerlink" href="#phase-3-running-the-full-371b-parameter-model" title="Permanent link">&para;</a></h2>
<p>Talk about the Azure model</p>
<p>Show the result.</p>

                
                <H2>Related Posts</H2>
                <div class="related-posts row">
                    <div class="col-md-4">
                        <a href="/posts/cjk-chinese-japanese-korean-llm-ai-best-practices.html">
                        <div class="related-box">
                            <img src="/img/posts-original/old_books.jpg" alt="Working with Chinese, Japanese, and Korean text in Generative AI pipelines" />
                            <div class="title">Working with Chinese, Japanese, and Korean text in Generative AI pipelines</div>
                            <div class="subheading">Best practices for working with CJK text in Large Language Models</div>
                            <div class="post-meta"><i>Published March 12, 2024</i></div>
                        </div>
                        </a>
                    </div>
                    <div class="col-md-4">
                        <a href="/posts/xss-exploitation-in-django.html">
                        <div class="related-box">
                            <img src="/img/posts/danger-snake.jpeg" alt="XSS Exploitation in Django Applications" />
                            <div class="title">XSS Exploitation in Django Applications</div>
                            <div class="subheading">A deep-dive on XSS exploitations in Django applications, how to find them and how to fix them</div>
                            <div class="post-meta"><i>Published July 22, 2020</i></div>
                        </div>
                        </a>
                    </div>
                    <div class="col-md-4">
                        <a href="/posts/performance-benchmarking-llm-models.html">
                        <div class="related-box">
                            <img src="/img/posts/llama-race.jpg" alt="Performance Benchmarking LLM Models" />
                            <div class="title">Performance Benchmarking LLM Models</div>
                            <div class="subheading">Using the LLM CLI to compare latency and throughput of multiple LLMs</div>
                            <div class="post-meta"><i>Published August 27, 2025</i></div>
                        </div>
                        </a>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <!-- Footer -->
    <footer>
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <ul class="list-inline text-center">
                        <li>
                            <a href="https://twitter.com/anthonypjshaw">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                        <li>
                            <a href="https://youtube.com/c/AnthonyShaw">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-youtube fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/tonybaloney">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                    </ul>
                    <p class="copyright text-muted">Copyright &copy; Anthony Shaw</p>
                </div>
            </div>
        </div>
    </footer>

    <!-- jQuery -->
    <script src="/vendor/jquery/jquery.min.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="/vendor/bootstrap/js/bootstrap.min.js"></script>

    <!-- Theme JavaScript -->
    <script src="/js/clean-blog.min.js"></script>

    <script>hljs.initHighlightingOnLoad();</script>

    <script>
    (function() {
        // Setup the toggle behavior and persistence. If the user hasn't chosen a preference, the site
        // follows the system preference (prefers-color-scheme). Stored choice overrides system changes.
        var toggle = document.getElementById('theme-toggle');
        var icon = toggle ? toggle.querySelector('i') : null;

        function getStored() {
            try { return localStorage.getItem('theme'); } catch (e) { return null; }
        }
        function setStored(t) {
            try { localStorage.setItem('theme', t); } catch (e) { }
        }
        function systemPrefersDark() {
            return window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
        }
        function updateIcon(t) {
            if (!icon) return;
            // sun when dark to indicate clicking will switch to light; moon when light to switch to dark
            icon.className = t === 'dark' ? 'fa fa-2x fa-sun-o' : 'fa fa-2x fa-moon-o';
        }
        function setTheme(t, save) {
            document.documentElement.setAttribute('data-theme', t);
            updateIcon(t);
            if (save) setStored(t);
        }

        if (toggle) {
            toggle.addEventListener('click', function(e) {
                e.preventDefault();
                var current = document.documentElement.getAttribute('data-theme') || (systemPrefersDark() ? 'dark' : 'light');
                var next = current === 'dark' ? 'light' : 'dark';
                setTheme(next, true);
            });
            // initialize icon from the attribute (which was set early in the head)
            var initial = document.documentElement.getAttribute('data-theme') || (getStored() || (systemPrefersDark() ? 'dark' : 'light'));
            updateIcon(initial);
        }

        // If the user hasn't explicitly set a preference, follow system changes.
        try {
            var mq = window.matchMedia('(prefers-color-scheme: dark)');
            var onChange = function(e) {
                if (!getStored()) {
                    setTheme(e.matches ? 'dark' : 'light', false);
                }
            };
            if (mq.addEventListener) mq.addEventListener('change', onChange);
            else if (mq.addListener) mq.addListener(onChange);
        } catch (e) { }
    })();
    </script>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PNMEXCY7DV"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-PNMEXCY7DV');
    </script>
</body>
</html>